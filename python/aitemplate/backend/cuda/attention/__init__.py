# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.
"""
cuda flash_attention module init
"""
from . import flash_attention

__all__ = ["flash_attention"]
